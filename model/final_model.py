# -*- coding: utf-8 -*-
"""final_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HCzH0imTbXz7RXZx7VaztDUrNB_VVJP5
"""

# !sudo apt-get install -y fonts-nanum
# !sudo fc-cache -fv
# !rm ~/.cache/matplotlib -rf
# !pip install konlpy

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
# import urllib.request
from konlpy.tag import Okt
# from tqdm import tqdm
import tensorflow as tf
# from tensorflow.keras.preprocessing.text import Tokenizer
# from tensorflow.keras.preprocessing.sequence import pad_sequences
# from sklearn.model_selection import train_test_split

# !pip install git+https://github.com/ssut/py-hanspell.git
from hanspell import spell_checker

#문장구분
# !pip install kss
import kss

# from google.colab import drive
# drive.mount('/content/drive')

"""# **전처리**"""

# 중복 제거

def duplicatesRemove(data) :
  data.drop_duplicates(subset = ['광고내용'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거
  data['광고내용'] = data['광고내용'].str.replace("[^가-힣 ]","") # 정규 표현식 수행
  data['광고내용'] = data['광고내용'].str.replace('^ +', "") # 공백은 empty 값으로 변경
  data['광고내용'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경

  data = data.dropna(how='any') # Null 값 제거

  data.reset_index(drop=True, inplace=True)

  print('전처리 후 데이터 수 :',len(data))

  return data

#hanspell 맞춤법 검사

def spellcheck(data):
  data['hanspell']=''
  for i in range(len(data)):
    spelled_sent = spell_checker.check(data["광고내용"][i])
    data['hanspell'][i] = spelled_sent.checked
  data.drop("광고내용", axis = 1, inplace = True)
  data = data[['hanspell','label']]
  data.rename(columns={'hanspell':'광고내용'}, inplace = True)
  return data

#토큰화+불용어제거
def ko_processing(df):
  stopwords = ['수','로','로부터','되다','다','든지','께','께서', '이라고','이다','것','의','가','하고','하고는','이','은','이므로','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','을']
  #stopwords = ['되다','인', '줄', '되어다', '것', '이라고','께','께서','든','든지','로','더러','며','만은','조차','처럼','한테','하고','하고는','커녕','나','이다','지요','이므로','있다','이다','의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','을']
  okt = Okt()
  df['ko_processing'] = np.nan
  df["ko_processing"] = df["ko_processing"].astype(object)

  X = []
  for sentence in df['광고내용']:
      tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화
      stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
      X.append(stopwords_removed_sentence)

  #ko_procssing
  for i in range(len(df)):
      df['ko_processing'][i]=X[i][:]
  df = df.dropna(how='any') # Null 값 제거
  return df

"""# **train/test**"""

#정수 인코딩
def processing(df):
  df["processing"] = np.nan
  df["processing"] = df["processing"].astype(object)


  X = df['ko_processing'].to_list()
  processing = tokenizer.texts_to_sequences(X)

  #procssing
  for i in range(len(df)):
    df['processing'][i]=processing[i][:]
  
  df = df.dropna(how='any') # Null 값 제거
  return df

"""# **패딩**"""

def tokenizing(df):
  X = []
  for sen in df['processing']:#processing에서 한줄을 가져옴
      word_arr =[]
      for word in sen:#[54,23,14]
          if word in word_index:
              word_arr.append(float(con[con[:,0] ==word,1])) #array([-0.95]) toal_value 가 들어감
          else:
              word_arr.append(0) #없으면 0이 들어감 

      [word_arr.insert(0,0) for i in range(141-len(word_arr))] #패딩하기
      X.append(word_arr)

  X = pd.DataFrame(X)
  X = X.loc[:,:140]
  X= X.to_numpy()
  X = X.reshape(-1,141,1)
  X.shape
  return X

"""# **모델링**"""

# from tensorflow.keras.layers import Embedding, Dense, LSTM
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.models import load_model
# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

import numpy as np
import pandas as pd
# from keras import models, layers
# from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

# from matplotlib import font_manager, rc

"""# **모델 부르기**"""


MODEL_NAME = 'model/lstm_best_XAImodel.h5'


import pickle
loaded_model = tf.keras.models.load_model(MODEL_NAME)
with open('model/tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

con = np.load('model/con_save.npy')
word_index = [row[0] for row in con]
word_index = list(map(int, word_index))

names = [weight.name for layer in loaded_model.layers for weight in layer.weights]
weights = loaded_model.get_weights()

kernel_weights = weights[0]
recurrent_kernel_weights = weights[1]
bias = weights[2]

n = 1
units = 141  # LSTM layers  

Wi = kernel_weights[:, 0:units]
Wf = kernel_weights[:, units:2 * units]
Wc = kernel_weights[:, 2 * units:3 * units]
Wo = kernel_weights[:, 3 * units:]


Ui = recurrent_kernel_weights[:, 0:units]
Uf = recurrent_kernel_weights[:, units:2 * units]
Uc = recurrent_kernel_weights[:, 2 * units:3 * units]
Uo = recurrent_kernel_weights[:, 3 * units:]


bi = bias[0:units]
bf = bias[units:2 * units]
bc = bias[2 * units:3 * units]
bo = bias[3 * units:]

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tokenizing1(df):
  X = []
  for sen in df['processing']:#processing에서 한줄을 가져옴
      word_arr =[]
      for word in sen:#[54,23,14]
          if word in word_index:
              word_arr.append(float(con[con[:,0] ==word,1])) #array([-0.95]) toal_value 가 들어감
          else:
              word_arr.append(0) #없으면 0이 들어감 
              
      [word_arr.insert(0,0) for i in range(141-len(word_arr))] #패딩하기
      X.append(word_arr)

      
  X = pd.DataFrame(X)
  X= X.to_numpy()
  X = X.reshape(-1,141,1)
  X.shape
  return X

import matplotlib.font_manager as fm
path = 'model/NanumGothic.ttf'
best_font = fm.FontProperties(fname=path, size=50).get_name()
plt.rc('font', family='NanumGothic')

def make_plot(test_df, test_li, number, standard_score):
    
    ht_1 = np.zeros(n * units).reshape(n, units)
    Ct_1 = np.zeros(n * units).reshape(n, units)

    h_t_value = []
    influence_h_t_value = []
    #test_li=test_li[0] #이중리스트로 변환

    for t in range(0, len(test_li[number,:])):
        xt = np.array(test_li[number,t])
        ft = sigmoid(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf)  # forget gate

        influence_ft = (np.dot(ht_1, Uf))/(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf) * ft

        it = sigmoid(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi)  # input gate
        influence_it = (np.dot(ht_1, Ui))/(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi) * it

        ot = sigmoid(np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo)  # output gate
        influence_ot = np.dot(ht_1, Uo) / (np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo) * ot

        gt =  np.tanh(np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc)
        influence_gt =np.dot(ht_1, Uc) / (np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc) * gt

        Ct = ft * Ct_1 + it * gt
        influence_ct = influence_ft * Ct_1 + influence_it * influence_gt
        ht = ot * np.tanh(Ct)
        influence_ht = influence_ot * (influence_ct/Ct) * ht

        influence_h_t_value.append(influence_ht)

        ht_1 = ht  # hidden state, previous memory state
        Ct_1 = Ct  # cell state, previous carry state

        h_t_value.append(ht)

    influence_h_t_value.append(h_t_value[-1])
    for i in range(len(influence_h_t_value)-1,0,-1):
        influence_h_t_value[i] = influence_h_t_value[i] - influence_h_t_value[i-1]

    influence_h_t_value = influence_h_t_value[1:]
    impact_columns = np.dot(influence_h_t_value,weights[3]) + (weights[4]/units)
    print("\n문장 : " , test_df.loc[number, '문장'])
    
    #score = loaded_model.predict(test_li[number:number+1])[0][0]
    score = loaded_model.predict(test_li[number:number+1])[0][0]
    test_df.loc[number,"score"] = score
    if score > standard_score:
        ment = "PASS SENTENCE"
        b_color = 'azure' #green일 경우 허용 0.5넘었을 때
        t_color = "mediumblue"
    else:
        ment = "DANGER SENTENCE"
        b_color ='mistyrose'
        t_color = "red"

    fig = plt.figure(figsize=(15,3),facecolor=b_color)
    for k in range(len(test_df.loc[number,'ko_processing'])):
        s = test_df.loc[number,'ko_processing'][k]
        k1=len(impact_columns)+k-len(test_df.loc[number,'ko_processing'])
        print(k1)
        va = round(float(impact_columns[k1]),2)
        
        if va > 0.5:
            font1 = {'family':best_font,
                'color':  'darkblue',
                'weight': 'normal',
                'size': 16}
        elif va< -0.3:
            font1 = {'family':best_font,
                'color':  'red',
                'weight': 'normal',
                'size': 16}

        else:
            font1 = {'family':best_font,
                'color':  'black',
                'weight': 'normal',
                'size': 16}


        if k < 17:
            plt.rcParams['axes.unicode_minus'] =False
            plt.rc('font', family='NanumGothic')
            plt.text(s=s, x=k*0.7, y=0,fontdict=font1,va='center',ha='center')
            plt.text(s=va,x=k*0.7, y=-0.1,fontdict=font1,va='center',ha='center')
        elif k < 34:
            plt.rcParams['axes.unicode_minus'] =False
            plt.rc('font', family='NanumGothic')
            plt.text(s=s, x=k*0.7 - 17*0.7, y=-0.2,fontdict=font1,va='center',ha='center')
            plt.text(s=va,x=k*0.7- 17*0.7, y=-0.3,fontdict=font1,va='center',ha='center')
        else:
            plt.rcParams['axes.unicode_minus'] =False
            plt.rc('font', family='NanumGothic')
            plt.text(s=s, x=k*0.7 - 34*0.7, y=-0.4,fontdict=font1,va='center',ha='center')
            plt.text(s=va,x=k*0.7- 34*0.7, y=-0.5,fontdict=font1,va='center',ha='center')   

    plt.xlim(0,8)
    plt.ylim(-0.5,0.1)
    plt.axis('off')
    plt.title(ment, size = 20, color = t_color, pad = 15)
    plt.show()
    print("위험도 : {:.3f}".format(1-score))

def ad_predict(ad):
  #문장 분리
  li = []

  #맞춤법+전처리
  spelled_sent = spell_checker.check(ad)
  ad = spelled_sent.checked
  ad = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', ad)
  for sent in kss.split_sentences(ad):
    li.append(sent)
  ad_df = pd.DataFrame({"광고내용":li})
  ad_df = ko_processing(ad_df)
  ad_df = processing(ad_df)

  ad_df['score'] = np.nan
  ad_df["score"] = ad_df["score"].astype(float)
  ad_df.rename(columns = {'광고내용':'문장'}, inplace = True)

  #토큰화
  input = tokenizing1(ad_df)
  print("광고 내 문장")
  num = 1
  for sen in ad_df['문장']:
    print("문장"+str(num)+" : " + sen)
    num += 1

  #문장이 2개 이상
  if len(ad_df)>1:
    #모델 예측
    print("\n\n문장 별 예측 결과")
    for i in range(len(ad_df)):
      make_plot(ad_df, input, i, 0.3)

    

    #score
    danger = ad_df.loc[ad_df['score']<0.3].copy()
    danger.rename(columns = {'문장':'위험 문장'}, inplace = True)
    danger.rename(columns = {'score':'위험도'}, inplace = True)
    danger = danger.reset_index(drop=True)
    safety = ad_df.loc[ad_df['score']>=0.3].copy()  

    if len(danger)!=0:
      danger['위험도'] = 1-danger['위험도']
      print("\n최종 예측 결과 : 다음 문장 때문에 해당 광고는 {:.2f}% 확률로 허위광고입니다.\n".format((danger['위험도'].mean()) * 100))
      # display(danger.loc[:,['위험 문장', '위험도']])
      return danger.loc[:,['위험 문장', '위험도']]

    else:
      print("\n최종 예측 결과 : 해당 광고는 {:.2f}% 확률로 허용광고입니다.\n".format(safety['score'].mean() * 100))


  #문장이 1개
  else:
    print("\n\n문장 별 예측 결과")
    make_plot(ad_df, input,0, 0.5)
    ad_df = ad_df.reset_index(drop=True)
    if(ad_df.loc[0,'score'] > 0.5):
      print("\n최종 예측 결과 : 해당 광고는 {:.2f}% 확률로 허용광고입니다.\n".format(ad_df['score'][0] * 100))

    else:
      ad_df['score'] = 1-ad_df['score']
      print("\n최종 예측 결과 : 다음 문장 때문에 해당 광고는 {:.2f}% 확률로 허위광고입니다.\n".format((ad_df['score'][0]) * 100))
      ad_df.rename(columns = {'문장':'위험 문장'}, inplace = True)
      ad_df.rename(columns = {'score':'위험도'}, inplace = True)
      # display(ad_df.loc[:,['위험 문장', '위험도']])
      return ad_df.loc[:,['위험 문장', '위험도']]

"""

# **실험**"""

#허위
ad_predict('먹으면 키가 커져요.')

# #허위
# ad_predict('바르기만 해도 살이 빠져요')


